{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7qcqRgrzi+xA8XP0WIW2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suchith-nj/Projects/blob/main/01_langchain_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tzmqnMPY_4pI",
        "outputId": "2e190464-1bd5-4717-c367-925c4c560b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain[openai] in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain[openai])\n",
            "  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (0.4.38)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (6.0.3)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (1.0.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain[openai]) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain[openai]) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain[openai]) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain[openai]) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain[openai]) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain[openai]) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain[openai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain[openai]) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain[openai]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain[openai]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain[openai]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain[openai]) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain[openai]) (3.2.4)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai (from langchain[openai])\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (0.12.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[openai]) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[openai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[openai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain[openai]) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->langchain[openai]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->langchain[openai]) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->langchain[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->langchain[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain[openai]) (2024.11.6)\n",
            "Downloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.0.1\n",
            "    Uninstalling langchain-core-1.0.1:\n",
            "      Successfully uninstalled langchain-core-1.0.1\n",
            "  Attempting uninstall: langchain-openai\n",
            "    Found existing installation: langchain-openai 1.0.1\n",
            "    Uninstalling langchain-openai-1.0.1:\n",
            "      Successfully uninstalled langchain-openai-1.0.1\n",
            "Successfully installed langchain-core-0.3.79 langchain-openai-0.3.35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "b7a1fb7002d1498cae2c3831a62c3e40"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain[openai] python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import openai\n",
        "print(f\"✅ LangChain version: {langchain.__version__}\")\n",
        "print(f\"✅ OpenAI version: {openai.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFAFtt6qA25-",
        "outputId": "48bc74e1-f355-4e35-ca1b-e79187559708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LangChain version: 0.3.27\n",
            "✅ OpenAI version: 1.109.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Test connection\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(\"✅ API Connection Successful!\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zsE0FDY7A3rs",
        "outputId": "0d7ffcd8-479a-4def-f05c-509545633985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3794891632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Test connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m response = client.chat.completions.create(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://api.openai.com/v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0m_strict_response_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_strict_response_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         )\n\u001b[0;32m--> 758\u001b[0;31m         self._client = http_client or SyncHttpxClientWrapper(\n\u001b[0m\u001b[1;32m    759\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;31m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall openai -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxvE0qH4B9LN",
        "outputId": "95b2ba7a-5e47-47aa-e30e-59757dfd9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: openai 1.6.1\n",
            "Uninstalling openai-1.6.1:\n",
            "  Successfully uninstalled openai-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall openai-python -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtoDay-RC-91",
        "outputId": "76c885e4-9569-4692-8e2f-6cbf182ed79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping openai-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "HlMsJPIlDCvI",
        "outputId": "2a232cb7-18fb-4ed5-acba-e98178b5b6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-2.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.5.0-py3-none-any.whl (999 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.9/999.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.0.2 requires openai<2.0.0,>=1.6.1, but you have openai 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "5c607e1559b04eb9a901e196909815f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Test connection\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(\"✅ API Connection Successful!\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fufG9HQnDGTJ",
        "outputId": "03ac8d19-e22a-4f3f-cea9-3dea5b55654c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API Connection Successful!\n",
            "Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,  # Creativity level (0=deterministic, 1=creative)\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "# Simple test\n",
        "response = llm.invoke(\"What is machine learning?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLEQjOFeDqBC",
        "outputId": "59e1a526-f644-4229-f4af-d251eeb49ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed to do so. It focuses on the development of computer programs that can access data and use it to learn for themselves. Machine learning algorithms can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "VdVX0RO_o5Wg",
        "outputId": "865fe16b-7484-49d0-bd3c-56f039cd8b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain_openai)\n",
            "  Downloading langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.4.35)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.0.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.0.0 langchain_openai-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "3564f1d77f434ebda399e4024dc20572"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,  # Creativity level (0=deterministic, 1=creative)\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "# Simple test\n",
        "response = llm.invoke(\"What is machine learning?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6RZ7gus5pVMS",
        "outputId": "e2033930-1d7c-465d-e7ad-8beaf0fee3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'userdata' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3199266951.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m llm = ChatOpenAI(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'userdata' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# COMPONENT 2: Prompt Templates\n",
        "# ============================================\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a template\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant specialized in {domain}.\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = template.format_messages(\n",
        "    domain=\"Python programming\",\n",
        "    user_input=\"How do I read a CSV file?\"\n",
        ")\n",
        "print('Hi')\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvq3IZEXpcLw",
        "outputId": "f9068d3b-6c1b-4431-b5cb-d6aaa4f7029d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n",
            "[SystemMessage(content='You are a helpful AI assistant specialized in Python programming.', additional_kwargs={}, response_metadata={}), HumanMessage(content='How do I read a CSV file?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SIMPLE CHATBOT - Traditional LLMChain Style\n",
        "# ============================================\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import Dict, Any\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets and set it as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "class SimpleChatbot:\n",
        "    \"\"\"\n",
        "    Production-grade simple chatbot without memory.\n",
        "    Uses traditional LLMChain for explicit control.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"gpt-3.5-turbo\",\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: int = 500\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize chatbot with configurable parameters.\n",
        "\n",
        "        Args:\n",
        "            model: OpenAI model name\n",
        "            temperature: Creativity level (0-1)\n",
        "            max_tokens: Maximum response length\n",
        "        \"\"\"\n",
        "        # Validate API key\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
        "\n",
        "        # Initialize LLM with production settings\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            request_timeout=30,  # Prevent hanging requests\n",
        "            max_retries=3  # Retry on API failures\n",
        "        )\n",
        "\n",
        "        # Define system prompt\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", self._get_system_prompt()),\n",
        "            (\"user\", \"{input}\")\n",
        "        ])\n",
        "\n",
        "        # Create chain\n",
        "        self.chain = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=self.prompt,\n",
        "            output_key=\"response\",\n",
        "            verbose=False  # Set True for debugging\n",
        "        )\n",
        "\n",
        "    def _get_system_prompt(self) -> str:\n",
        "        \"\"\"\n",
        "        Define bot personality and behavior.\n",
        "        Separating this makes it easy to modify.\n",
        "        \"\"\"\n",
        "        return \"\"\"You are a friendly and helpful AI assistant.\n",
        "\n",
        "              Guidelines:\n",
        "              - Keep responses concise and clear\n",
        "              - Be professional but warm\n",
        "              - If you don't know something, admit it\n",
        "              - Never make up information\n",
        "              - Ask clarifying questions when needed\n",
        "              \"\"\"\n",
        "\n",
        "    def chat(self, user_input: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process user input and return response.\n",
        "\n",
        "        Args:\n",
        "            user_input: User's message\n",
        "\n",
        "        Returns:\n",
        "            Dict with response and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validate input\n",
        "            if not user_input or not user_input.strip():\n",
        "                return {\n",
        "                    \"response\": \"I didn't receive any message. Please try again.\",\n",
        "                    \"error\": None\n",
        "                }\n",
        "\n",
        "            # Get response from chain\n",
        "            result = self.chain.invoke({\"input\": user_input.strip()})\n",
        "\n",
        "            return {\n",
        "                \"response\": result[\"response\"],\n",
        "                \"error\": None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle errors gracefully\n",
        "            return {\n",
        "                \"response\": \"I encountered an error. Please try again.\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return current configuration for debugging.\"\"\"\n",
        "        return {\n",
        "            \"model\": self.llm.model_name,\n",
        "            \"temperature\": self.llm.temperature,\n",
        "            \"max_tokens\": self.llm.max_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TESTING\n",
        "# ============================================\n",
        "\n",
        "# Initialize bot\n",
        "bot = SimpleChatbot(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Print configuration\n",
        "print(\"Bot Configuration:\")\n",
        "print(bot.get_config())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Test conversations\n",
        "test_messages = [\n",
        "    \"Hello! How are you?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"\",  # Test empty input\n",
        "    \"Explain it in simple terms\"\n",
        "]\n",
        "\n",
        "# for msg in test_messages:\n",
        "#     print(f\"You: {msg if msg else '(empty)'}\")\n",
        "#     result = bot.chat(msg)\n",
        "#     print(f\"Bot: {result['response']}\")\n",
        "#     if result['error']:\n",
        "#         print(f\"Error: {result['error']}\")\n",
        "#     print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "k_-XvgM6t8qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2ea08c-2716-4e59-9cf5-1dd6095ade9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2783509239.py:56: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  self.chain = LLMChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot Configuration:\n",
            "{'model': 'gpt-3.5-turbo', 'temperature': 0.7, 'max_tokens': 500}\n",
            "\n",
            "==================================================\n",
            "\n",
            "You: Hello! How are you?\n",
            "Bot: Hello! I'm here and ready to assist you. How can I help you today?\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "You: What is machine learning?\n",
            "Bot: Machine learning is a type of artificial intelligence that enables computers to learn from data. It involves developing algorithms that allow computers to recognize patterns in data and make predictions or decisions without being explicitly programmed.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "You: (empty)\n",
            "Bot: I didn't receive any message. Please try again.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "You: Explain it in simple terms\n",
            "Bot: Of course! Please let me know what topic or information you would like me to explain in simple terms.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import Dict, List\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BufferMemoryChatbot:\n",
        "    \"\"\"\n",
        "    Production chatbot using .invoke() (modern API).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"gpt-3.5-turbo\",\n",
        "        temperature: float = 0.7,\n",
        "        verbose: bool = False\n",
        "    ):\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=500,\n",
        "            request_timeout=30\n",
        "        )\n",
        "\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            return_messages=True,\n",
        "            memory_key=\"history\",\n",
        "            input_key=\"input\",\n",
        "            output_key=\"response\"\n",
        "        )\n",
        "\n",
        "        self.conversation = ConversationChain(\n",
        "            llm=self.llm,\n",
        "            memory=self.memory,\n",
        "            verbose=verbose,\n",
        "            input_key=\"input\",\n",
        "            output_key=\"response\"\n",
        "        )\n",
        "\n",
        "    def chat(self, user_input: str) -> Dict:\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            user_input: User's message\n",
        "\n",
        "        Returns:\n",
        "            Dict with response and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not user_input or not user_input.strip():\n",
        "                return {\n",
        "                    \"response\": \"Please provide a message.\",\n",
        "                    \"error\": \"empty_input\"\n",
        "                }\n",
        "\n",
        "\n",
        "            result = self.conversation.invoke({\"input\": user_input.strip()})\n",
        "\n",
        "            # result is a dict: {\"input\": \"...\", \"response\": \"...\", \"history\": [...]}\n",
        "            # Extract response from dict\n",
        "            response_text = result[\"response\"]\n",
        "\n",
        "            # Get memory statistics\n",
        "            messages = self.memory.load_memory_variables({})\n",
        "\n",
        "            return {\n",
        "                \"response\": response_text,\n",
        "                \"error\": None,\n",
        "                \"metadata\": {\n",
        "                    \"message_count\": len(messages.get(\"history\", [])),\n",
        "                    \"memory_type\": \"buffer\",\n",
        "                    \"full_result\": result  # Include full result for debugging\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Chat error: {str(e)}\")\n",
        "            return {\n",
        "                \"response\": \"An error occurred. Please try again.\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def get_history(self) -> List:\n",
        "        \"\"\"Get conversation history.\"\"\"\n",
        "        memory_vars = self.memory.load_memory_variables({})\n",
        "        return memory_vars.get(\"history\", [])\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear conversation memory.\"\"\"\n",
        "        self.memory.clear()\n",
        "        logger.info(\"Memory cleared\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TESTING\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TESTING .invoke() METHOD\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "bot = BufferMemoryChatbot(verbose=True)\n",
        "\n",
        "result = bot.chat(\"My name is Suchith\")\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"Metadata: {result['metadata']}\\n\")\n",
        "\n",
        "result = bot.chat(\"What's my name?\")\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"Metadata: {result['metadata']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDZQYdjhC7yB",
        "outputId": "f5fcae08-145b-429e-a768-c6e3e30f20fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TESTING .invoke() METHOD\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: My name is Suchith\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-698458466.py:28: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.memory = ConversationBufferMemory(\n",
            "/tmp/ipython-input-698458466.py:35: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  self.conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: Hello Suchith! It's nice to meet you. How can I assist you today?\n",
            "Metadata: {'message_count': 2, 'memory_type': 'buffer', 'full_result': {'input': 'My name is Suchith', 'history': [HumanMessage(content='My name is Suchith', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Suchith! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={})], 'response': \"Hello Suchith! It's nice to meet you. How can I assist you today?\"}}\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='My name is Suchith', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Suchith! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: What's my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: Your name is Suchith.\n",
            "Metadata: {'message_count': 4, 'memory_type': 'buffer', 'full_result': {'input': \"What's my name?\", 'history': [HumanMessage(content='My name is Suchith', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Suchith! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Suchith.', additional_kwargs={}, response_metadata={})], 'response': 'Your name is Suchith.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PostgreSQL\n",
        "!apt-get update\n",
        "!apt-get install postgresql postgresql-contrib -y\n",
        "\n",
        "# Start PostgreSQL service\n",
        "!service postgresql start\n",
        "\n",
        "# Install Python packages\n",
        "!pip install psycopg2-binary sqlalchemy python-dotenv\n",
        "\n",
        "# Verify installation\n",
        "!sudo -u postgres psql --version\n",
        "\n",
        "print(\"✅ PostgreSQL installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xu_JAriTK13R",
        "outputId": "fe0f8035-19ee-4a44-a911-1a072bd84cbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r                                                                               \rHit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,412 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,479 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,820 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,838 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,135 kB]\n",
            "Fetched 29.0 MB in 3s (9,004 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common ssl-cert\n",
            "  sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common\n",
            "  postgresql-contrib ssl-cert sysstat\n",
            "0 upgraded, 15 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 42.4 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjson-xs-perl amd64 4.040-0ubuntu0.22.04.1 [87.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm14 amd64 1:14.0.0-1ubuntu1.1 [24.0 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.19-0ubuntu0.22.04.1 [1,249 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.19-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 42.4 MB in 3s (15.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 125079 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libllvm14:amd64.\n",
            "Preparing to unpack .../06-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../07-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../08-postgresql-client-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../09-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../10-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../11-postgresql-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../12-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../13-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../14-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.44)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
            "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psycopg2"
                ]
              },
              "id": "c8c44f3488cd4a0b9056f53a62fff9e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "psql (PostgreSQL) 14.19 (Ubuntu 14.19-0ubuntu0.22.04.1)\n",
            "✅ PostgreSQL installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CREATE DATABASE AND USER\n",
        "# ============================================\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Create database and user\n",
        "commands = [\n",
        "    # Create database\n",
        "    \"sudo -u postgres psql -c \\\"CREATE DATABASE chatbot_db;\\\"\",\n",
        "\n",
        "    # Create user with password\n",
        "    \"sudo -u postgres psql -c \\\"CREATE USER chatbot_user WITH PASSWORD 'secure_password_123';\\\"\",\n",
        "\n",
        "    # Grant privileges\n",
        "    \"sudo -u postgres psql -c \\\"GRANT ALL PRIVILEGES ON DATABASE chatbot_db TO chatbot_user;\\\"\",\n",
        "\n",
        "    # Grant schema privileges\n",
        "    \"sudo -u postgres psql -d chatbot_db -c \\\"GRANT ALL ON SCHEMA public TO chatbot_user;\\\"\"\n",
        "]\n",
        "\n",
        "for cmd in commands:\n",
        "    try:\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"✅ Success: {cmd[:50]}...\")\n",
        "        else:\n",
        "            print(f\"⚠️  Warning: {result.stderr}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "print(\"\\n✅ Database setup complete!\")\n",
        "print(\"Database: chatbot_db\")\n",
        "print(\"User: chatbot_user\")\n",
        "print(\"Password: secure_password_123\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wklNzqQEK2rP",
        "outputId": "910c281b-63e3-4442-d0ec-1de05207de4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Success: sudo -u postgres psql -c \"CREATE DATABASE chatbot_...\n",
            "✅ Success: sudo -u postgres psql -c \"CREATE USER chatbot_user...\n",
            "✅ Success: sudo -u postgres psql -c \"GRANT ALL PRIVILEGES ON ...\n",
            "✅ Success: sudo -u postgres psql -d chatbot_db -c \"GRANT ALL ...\n",
            "\n",
            "✅ Database setup complete!\n",
            "Database: chatbot_db\n",
            "User: chatbot_user\n",
            "Password: secure_password_123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST DATABASE CONNECTION\n",
        "# ============================================\n",
        "\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "\n",
        "def test_connection():\n",
        "    \"\"\"Test PostgreSQL connection.\"\"\"\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            host=\"localhost\",\n",
        "            database=\"chatbot_db\",\n",
        "            user=\"chatbot_user\",\n",
        "            password=\"secure_password_123\"\n",
        "        )\n",
        "\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT version();\")\n",
        "        version = cursor.fetchone()\n",
        "\n",
        "        print(\"✅ Connection successful!\")\n",
        "        print(f\"PostgreSQL version: {version[0][:50]}...\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Connection failed: {e}\")\n",
        "        return False\n",
        "\n",
        "test_connection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi-LmrsiLWvm",
        "outputId": "1e5d37bd-922d-46fc-e940-956ce3060bb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Connection successful!\n",
            "PostgreSQL version: PostgreSQL 14.19 (Ubuntu 14.19-0ubuntu0.22.04.1) o...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SQL SCHEMA CREATION\n",
        "# ============================================\n",
        "\n",
        "import psycopg2\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "# SQL to create all tables\n",
        "CREATE_SCHEMA_SQL = \"\"\"\n",
        "-- Enable UUID extension\n",
        "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n",
        "\n",
        "-- Users table\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    username VARCHAR(255) UNIQUE NOT NULL,\n",
        "    email VARCHAR(255),\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Conversations table\n",
        "CREATE TABLE IF NOT EXISTS conversations (\n",
        "    conversation_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,\n",
        "    title VARCHAR(500),\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Messages table\n",
        "CREATE TABLE IF NOT EXISTS messages (\n",
        "    message_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    conversation_id UUID REFERENCES conversations(conversation_id) ON DELETE CASCADE,\n",
        "    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),\n",
        "    content TEXT NOT NULL,\n",
        "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    tokens_used INTEGER DEFAULT 0,\n",
        "    model VARCHAR(100)\n",
        ");\n",
        "\n",
        "-- Create indexes for better query performance\n",
        "CREATE INDEX IF NOT EXISTS idx_conversations_user_id ON conversations(user_id);\n",
        "CREATE INDEX IF NOT EXISTS idx_conversations_updated_at ON conversations(updated_at DESC);\n",
        "CREATE INDEX IF NOT EXISTS idx_messages_conversation_id ON messages(conversation_id);\n",
        "CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);\n",
        "\n",
        "-- Create function to auto-update updated_at\n",
        "CREATE OR REPLACE FUNCTION update_updated_at_column()\n",
        "RETURNS TRIGGER AS $$\n",
        "BEGIN\n",
        "    NEW.updated_at = CURRENT_TIMESTAMP;\n",
        "    RETURN NEW;\n",
        "END;\n",
        "$$ language 'plpgsql';\n",
        "\n",
        "-- Create trigger for conversations table\n",
        "DROP TRIGGER IF EXISTS update_conversations_updated_at ON conversations;\n",
        "CREATE TRIGGER update_conversations_updated_at\n",
        "    BEFORE UPDATE ON conversations\n",
        "    FOR EACH ROW\n",
        "    EXECUTE FUNCTION update_updated_at_column();\n",
        "\"\"\"\n",
        "\n",
        "def create_schema():\n",
        "    \"\"\"Create database schema.\"\"\"\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            host=\"localhost\",\n",
        "            database=\"chatbot_db\",\n",
        "            user=\"chatbot_user\",\n",
        "            password=\"secure_password_123\"\n",
        "        )\n",
        "\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(CREATE_SCHEMA_SQL)\n",
        "        conn.commit()\n",
        "\n",
        "        print(\"✅ Schema created successfully!\")\n",
        "\n",
        "        # Verify tables created\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT table_name\n",
        "            FROM information_schema.tables\n",
        "            WHERE table_schema = 'public'\n",
        "        \"\"\")\n",
        "\n",
        "        tables = cursor.fetchall()\n",
        "        print(f\"\\n📊 Tables created: {[t[0] for t in tables]}\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Schema creation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "create_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt1BmISELr6C",
        "outputId": "8e75932e-510f-4b95-932e-2571d54ae1d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Schema created successfully!\n",
            "\n",
            "📊 Tables created: ['users', 'conversations', 'messages']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SQL SCHEMA CREATION\n",
        "# ============================================\n",
        "\n",
        "import psycopg2\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "# SQL to create all tables\n",
        "CREATE_SCHEMA_SQL = \"\"\"\n",
        "-- Enable UUID extension\n",
        "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n",
        "\n",
        "-- Users table\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    username VARCHAR(255) UNIQUE NOT NULL,\n",
        "    email VARCHAR(255),\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Conversations table\n",
        "CREATE TABLE IF NOT EXISTS conversations (\n",
        "    conversation_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,\n",
        "    title VARCHAR(500),\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Messages table\n",
        "CREATE TABLE IF NOT EXISTS messages (\n",
        "    message_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
        "    conversation_id UUID REFERENCES conversations(conversation_id) ON DELETE CASCADE,\n",
        "    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),\n",
        "    content TEXT NOT NULL,\n",
        "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    tokens_used INTEGER DEFAULT 0,\n",
        "    model VARCHAR(100)\n",
        ");\n",
        "\n",
        "-- Create indexes for better query performance\n",
        "CREATE INDEX IF NOT EXISTS idx_conversations_user_id ON conversations(user_id);\n",
        "CREATE INDEX IF NOT EXISTS idx_conversations_updated_at ON conversations(updated_at DESC);\n",
        "CREATE INDEX IF NOT EXISTS idx_messages_conversation_id ON messages(conversation_id);\n",
        "CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);\n",
        "\n",
        "-- Create function to auto-update updated_at\n",
        "CREATE OR REPLACE FUNCTION update_updated_at_column()\n",
        "RETURNS TRIGGER AS $$\n",
        "BEGIN\n",
        "    NEW.updated_at = CURRENT_TIMESTAMP;\n",
        "    RETURN NEW;\n",
        "END;\n",
        "$$ language 'plpgsql';\n",
        "\n",
        "-- Create trigger for conversations table\n",
        "DROP TRIGGER IF EXISTS update_conversations_updated_at ON conversations;\n",
        "CREATE TRIGGER update_conversations_updated_at\n",
        "    BEFORE UPDATE ON conversations\n",
        "    FOR EACH ROW\n",
        "    EXECUTE FUNCTION update_updated_at_column();\n",
        "\"\"\"\n",
        "\n",
        "def create_schema():\n",
        "    \"\"\"Create database schema.\"\"\"\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            host=\"localhost\",\n",
        "            database=\"chatbot_db\",\n",
        "            user=\"chatbot_user\",\n",
        "            password=\"secure_password_123\"\n",
        "        )\n",
        "\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(CREATE_SCHEMA_SQL)\n",
        "        conn.commit()\n",
        "\n",
        "        print(\"✅ Schema created successfully!\")\n",
        "\n",
        "        # Verify tables created\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT table_name\n",
        "            FROM information_schema.tables\n",
        "            WHERE table_schema = 'public'\n",
        "        \"\"\")\n",
        "\n",
        "        tables = cursor.fetchall()\n",
        "        print(f\"\\n📊 Tables created: {[t[0] for t in tables]}\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Schema creation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "create_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtSemv_UMHKD",
        "outputId": "5e9c99e0-2588-489c-edef-45eb912ce646"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Schema created successfully!\n",
            "\n",
            "📊 Tables created: ['users', 'conversations', 'messages']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SQLALCHEMY ORM MODELS\n",
        "# ============================================\n",
        "\n",
        "from sqlalchemy import create_engine, Column, String, Integer, Text, DateTime, ForeignKey\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import relationship, sessionmaker\n",
        "from sqlalchemy.dialects.postgresql import UUID\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "# Create base class\n",
        "Base = declarative_base()\n",
        "\n",
        "# ============================================\n",
        "# MODEL DEFINITIONS\n",
        "# ============================================\n",
        "\n",
        "class User(Base):\n",
        "    \"\"\"User model.\"\"\"\n",
        "    __tablename__ = 'users'\n",
        "\n",
        "    user_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
        "    username = Column(String(255), unique=True, nullable=False)\n",
        "    email = Column(String(255))\n",
        "    created_at = Column(DateTime, default=datetime.utcnow)\n",
        "\n",
        "    # Relationships\n",
        "    conversations = relationship(\"Conversation\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<User(username='{self.username}')>\"\n",
        "\n",
        "\n",
        "class Conversation(Base):\n",
        "    \"\"\"Conversation model.\"\"\"\n",
        "    __tablename__ = 'conversations'\n",
        "\n",
        "    conversation_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
        "    user_id = Column(UUID(as_uuid=True), ForeignKey('users.user_id', ondelete='CASCADE'))\n",
        "    title = Column(String(500))\n",
        "    created_at = Column(DateTime, default=datetime.utcnow)\n",
        "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
        "\n",
        "    # Relationships\n",
        "    user = relationship(\"User\", back_populates=\"conversations\")\n",
        "    messages = relationship(\"Message\", back_populates=\"conversation\", cascade=\"all, delete-orphan\", order_by=\"Message.timestamp\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Conversation(title='{self.title}', messages={len(self.messages)})>\"\n",
        "\n",
        "\n",
        "class Message(Base):\n",
        "    \"\"\"Message model.\"\"\"\n",
        "    __tablename__ = 'messages'\n",
        "\n",
        "    message_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
        "    conversation_id = Column(UUID(as_uuid=True), ForeignKey('conversations.conversation_id', ondelete='CASCADE'))\n",
        "    role = Column(String(20), nullable=False)  # 'user' or 'assistant'\n",
        "    content = Column(Text, nullable=False)\n",
        "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
        "    tokens_used = Column(Integer, default=0)\n",
        "    model = Column(String(100))\n",
        "\n",
        "    # Relationships\n",
        "    conversation = relationship(\"Conversation\", back_populates=\"messages\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Message(role='{self.role}', content='{self.content[:30]}...')>\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# DATABASE CONNECTION MANAGER\n",
        "# ============================================\n",
        "\n",
        "class DatabaseManager:\n",
        "    \"\"\"\n",
        "    Manages database connections and sessions.\n",
        "    Production-ready with connection pooling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, database_url: str):\n",
        "        \"\"\"\n",
        "        Initialize database manager.\n",
        "\n",
        "        Args:\n",
        "            database_url: PostgreSQL connection string\n",
        "                Format: postgresql://user:password@host:port/database\n",
        "        \"\"\"\n",
        "        self.engine = create_engine(\n",
        "            database_url,\n",
        "            pool_size=10,  # Connection pool size\n",
        "            max_overflow=20,  # Max connections beyond pool_size\n",
        "            pool_pre_ping=True,  # Verify connections before using\n",
        "            echo=False  # Set True for SQL logging\n",
        "        )\n",
        "\n",
        "        self.SessionLocal = sessionmaker(\n",
        "            autocommit=False,\n",
        "            autoflush=False,\n",
        "            bind=self.engine\n",
        "        )\n",
        "\n",
        "    def create_tables(self):\n",
        "        \"\"\"Create all tables.\"\"\"\n",
        "        Base.metadata.create_all(bind=self.engine)\n",
        "        print(\"✅ Tables created via SQLAlchemy\")\n",
        "\n",
        "    def drop_tables(self):\n",
        "        \"\"\"Drop all tables (use with caution!).\"\"\"\n",
        "        Base.metadata.drop_all(bind=self.engine)\n",
        "        print(\"⚠️  All tables dropped\")\n",
        "\n",
        "    def get_session(self):\n",
        "        \"\"\"Get a new database session.\"\"\"\n",
        "        return self.SessionLocal()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection.\"\"\"\n",
        "        self.engine.dispose()\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# INITIALIZE DATABASE\n",
        "# ============================================\n",
        "\n",
        "# Create database manager\n",
        "DATABASE_URL = \"postgresql://chatbot_user:secure_password_123@localhost/chatbot_db\"\n",
        "db_manager = DatabaseManager(DATABASE_URL)\n",
        "\n",
        "# Create tables\n",
        "db_manager.create_tables()\n",
        "\n",
        "print(\"✅ SQLAlchemy models initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCHZvItGWjXE",
        "outputId": "29f18fe5-5e59-4e7f-8a6f-50bba1ff3bc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tables created via SQLAlchemy\n",
            "✅ SQLAlchemy models initialized!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3467961004.py:13: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base = declarative_base()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST MODELS - INSERT SAMPLE DATA\n",
        "# ============================================\n",
        "\n",
        "def test_models():\n",
        "    \"\"\"Test ORM models with sample data.\"\"\"\n",
        "\n",
        "    # Get session\n",
        "    session = db_manager.get_session()\n",
        "\n",
        "    try:\n",
        "        # 1. Create user\n",
        "        user = User(\n",
        "            username=\"test_user\",\n",
        "            email=\"test@example.com\"\n",
        "        )\n",
        "        session.add(user)\n",
        "        session.commit()\n",
        "        print(f\"✅ Created user: {user.user_id}\")\n",
        "\n",
        "        # 2. Create conversation\n",
        "        conversation = Conversation(\n",
        "            user_id=user.user_id,\n",
        "            title=\"My first conversation\"\n",
        "        )\n",
        "        session.add(conversation)\n",
        "        session.commit()\n",
        "        print(f\"✅ Created conversation: {conversation.conversation_id}\")\n",
        "\n",
        "        # 3. Add messages\n",
        "        messages = [\n",
        "            Message(\n",
        "                conversation_id=conversation.conversation_id,\n",
        "                role=\"user\",\n",
        "                content=\"Hello! What is machine learning?\",\n",
        "                tokens_used=10,\n",
        "                model=\"gpt-3.5-turbo\"\n",
        "            ),\n",
        "            Message(\n",
        "                conversation_id=conversation.conversation_id,\n",
        "                role=\"assistant\",\n",
        "                content=\"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
        "                tokens_used=25,\n",
        "                model=\"gpt-3.5-turbo\"\n",
        "            ),\n",
        "            Message(\n",
        "                conversation_id=conversation.conversation_id,\n",
        "                role=\"user\",\n",
        "                content=\"Can you explain it more simply?\",\n",
        "                tokens_used=8,\n",
        "                model=\"gpt-3.5-turbo\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        session.add_all(messages)\n",
        "        session.commit()\n",
        "        print(f\"✅ Added {len(messages)} messages\")\n",
        "\n",
        "        # 4. Query data back\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"QUERYING DATA:\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Get conversation with messages\n",
        "        conv = session.query(Conversation).filter_by(\n",
        "            conversation_id=conversation.conversation_id\n",
        "        ).first()\n",
        "\n",
        "        print(f\"\\nConversation: {conv.title}\")\n",
        "        print(f\"User: {conv.user.username}\")\n",
        "        print(f\"Created: {conv.created_at}\")\n",
        "        print(f\"Message count: {len(conv.messages)}\\n\")\n",
        "\n",
        "        print(\"Messages:\")\n",
        "        for msg in conv.messages:\n",
        "            print(f\"  [{msg.role}]: {msg.content[:50]}...\")\n",
        "            print(f\"    Tokens: {msg.tokens_used}, Model: {msg.model}\\n\")\n",
        "\n",
        "        # 5. Calculate total tokens\n",
        "        total_tokens = sum(msg.tokens_used for msg in conv.messages)\n",
        "        print(f\"Total tokens used: {total_tokens}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        session.rollback()\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        session.close()\n",
        "\n",
        "test_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUZcoRPeXRz2",
        "outputId": "c9782d13-2dc7-4d6c-9b19-570f3ea51f97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created user: edccd12b-ab88-483f-b201-d90f6ae416cd\n",
            "✅ Created conversation: e431e22a-4f97-4063-b5ef-ad839e0443c4\n",
            "✅ Added 3 messages\n",
            "\n",
            "============================================================\n",
            "QUERYING DATA:\n",
            "============================================================\n",
            "\n",
            "Conversation: My first conversation\n",
            "User: test_user\n",
            "Created: 2025-10-31 03:44:29.176619\n",
            "Message count: 3\n",
            "\n",
            "Messages:\n",
            "  [user]: Hello! What is machine learning?...\n",
            "    Tokens: 10, Model: gpt-3.5-turbo\n",
            "\n",
            "  [assistant]: Machine learning is a subset of AI that enables sy...\n",
            "    Tokens: 25, Model: gpt-3.5-turbo\n",
            "\n",
            "  [user]: Can you explain it more simply?...\n",
            "    Tokens: 8, Model: gpt-3.5-turbo\n",
            "\n",
            "Total tokens used: 43\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}